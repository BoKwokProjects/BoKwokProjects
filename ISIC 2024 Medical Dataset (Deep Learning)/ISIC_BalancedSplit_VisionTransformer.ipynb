{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39882194"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "from collections import namedtuple\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import collections\n",
    "import math\n",
    "import pandas as pd\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6061aGw4pM0w"
   },
   "source": [
    "If you want the result to be reproducible, you will need to set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrPC2sxvRkvX"
   },
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSV0OKw4pY14"
   },
   "source": [
    "Mount to Google drive and unpack your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-h6147Klo6y",
    "outputId": "276dceb5-b51a-4f3d-834f-e8baf9c5f049"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJPQgFTuoRuf"
   },
   "outputs": [],
   "source": [
    "#import shutil\n",
    "shutil.unpack_archive(\"/content/drive/MyDrive/Deep Learning/ISIC_balanced_split.zip\", \"/content/drive/MyDrive/Deep Learning/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIKPVr-XyUL6"
   },
   "source": [
    "Run calculation on mean and standard deviation of my train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjv-NBDLp4No"
   },
   "source": [
    "This is an additional code to check the mean and standard deviation of your dataset - you can then use it to replace the values in transforms.Normalize to improve your work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nbsqpyy1gFze",
    "outputId": "94a61768-9b84-4bec-889b-117c50bc3c81"
   },
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(root = '/content/drive/My Drive/Deep Learning/ISIC_balanced_split/train/',\n",
    "                                  transform = transforms.ToTensor())\n",
    "\n",
    "means = torch.zeros(3)\n",
    "stds = torch.zeros(3)\n",
    "\n",
    "for img, label in train_data:\n",
    "    means += torch.mean(img, dim = (1,2))\n",
    "    stds += torch.std(img, dim = (1,2))\n",
    "\n",
    "means /= len(train_data)\n",
    "stds /= len(train_data)\n",
    "\n",
    "print(f'Calculated means: {means}')\n",
    "print(f'Calculated stds: {stds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYSwCw3Rpsx6"
   },
   "source": [
    "Image resizing and image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3c34f65"
   },
   "outputs": [],
   "source": [
    "# set batch size to 32\n",
    "batch_size = 32\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n",
    "    # the original area and height-to-width ratio between 3/4 and 4/3. Then,\n",
    "    # scale the image to create a new 224 x 224 image\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Randomly change the brightness, contrast, and saturation\n",
    "    transforms.ColorJitter(brightness=0.4,\n",
    "                                        contrast=0.4,\n",
    "                                        saturation=0.4),\n",
    "    # Add random noise\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.1),\n",
    "\n",
    "    # Standardize each channel of the image\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "    #                                 [0.229, 0.224, 0.225])\n",
    "    transforms.Normalize([0.6938, 0.5495, 0.5302],[0.1293, 0.1447, 0.1579])\n",
    "   ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    # Crop a 224 x 224 square area from the center of the image\n",
    "    #torchvision.transforms.CenterCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406],\n",
    "    #                                 [0.229, 0.224, 0.225])\n",
    "    transforms.Normalize([0.6938, 0.5495, 0.5302],[0.1293, 0.1447, 0.1579])\n",
    "   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeeGwNRet7I6"
   },
   "source": [
    "Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EilX_plpt9L5"
   },
   "outputs": [],
   "source": [
    "data_dir = '/content/drive/My Drive/Deep Learning/ISIC_balanced_split/'\n",
    "\n",
    "train_ds, train_valid_ds = [datasets.ImageFolder(\n",
    "    os.path.join(data_dir, folder),\n",
    "    transform=transform_train) for folder in ['train', 'valid']]\n",
    "\n",
    "valid_ds, test_ds = [datasets.ImageFolder(\n",
    "    os.path.join(data_dir, folder),\n",
    "    transform=transform_test) for folder in ['valid', 'ISIC2017_test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoV946Fy6azH"
   },
   "outputs": [],
   "source": [
    "train_iter, train_valid_iter = [torch.utils.data.DataLoader(\n",
    "    dataset, batch_size, shuffle=True, drop_last=True)\n",
    "    for dataset in (train_ds, valid_ds)]\n",
    "\n",
    "valid_iter = torch.utils.data.DataLoader(valid_ds, batch_size, shuffle=False,\n",
    "                                         drop_last=True)\n",
    "\n",
    "test_iter = torch.utils.data.DataLoader(test_ds, batch_size, shuffle=False,\n",
    "                                        drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TN7aGI-S3JHm"
   },
   "source": [
    "Show image - display some of your images for observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UW0sqDsRxI1"
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def plot_images(images, labels, classes, normalize = True):\n",
    "\n",
    "    n_images = len(images)\n",
    "\n",
    "    rows = int(np.sqrt(n_images))\n",
    "    cols = int(np.sqrt(n_images))\n",
    "\n",
    "    fig = plt.figure(figsize = (15, 15))\n",
    "\n",
    "    for i in range(rows*cols):\n",
    "\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "\n",
    "        image = images[i]\n",
    "\n",
    "        if normalize:\n",
    "            image = normalize_image(image)\n",
    "\n",
    "        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "        label = classes[labels[i]]\n",
    "        ax.set_title(label)\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rucCMNrqNsX9",
    "outputId": "7ef30cb3-95cf-4bda-c9eb-bb531be29265"
   },
   "outputs": [],
   "source": [
    "N_IMAGES = 10\n",
    "j=int(len(train_ds)/2) - int(N_IMAGES/2)\n",
    "\n",
    "images, labels = zip(*[(image, label) for image, label in\n",
    "                           [train_ds[i+j] for i in range(N_IMAGES)]])\n",
    "\n",
    "classes = train_ds.classes\n",
    "\n",
    "plot_images(images, labels, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NETRAKM8qW4v"
   },
   "source": [
    "Define the CNN - this example will focus on AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxq-Q9K9kzBw"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, num_layers=12, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = in_channels * patch_size * patch_size\n",
    "        self.patch_embed = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim)* 0.02)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)* 0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model= embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward= embed_dim * 4,\n",
    "            dropout=0.1,\n",
    "            activation='gelu'\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        patches = self.patch_embed(x)\n",
    "        patches = patches.flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(x.size(0), -1, -1)\n",
    "        tokens = torch.cat((cls_tokens, patches), dim=1)\n",
    "        tokens += self.positional_embeddings[:, :tokens.size(1), :]\n",
    "        tokens = tokens.permute(1, 0, 2)\n",
    "        encoded = self.transformer(tokens)\n",
    "\n",
    "        cls_output = encoded[0]\n",
    "        logits = self.mlp_head(cls_output)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTbbA3M4q6Y0"
   },
   "source": [
    "HERE you can adjust the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiOgETzglDMN",
    "outputId": "119bf83a-9a7a-47c2-b76d-3edd676887d5"
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIM = 2\n",
    "\n",
    "model = ViT(img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, num_layers=12, output_dim=OUTPUT_DIM)\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity='relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "model.apply(initialize_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy7KSZg3uUb8"
   },
   "source": [
    "This is to display the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wrROb1bETFDy",
    "outputId": "e2c5f200-e408-4e8c-f7ba-9d306840a1de"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5yE7GYedACj"
   },
   "source": [
    "Setting the hyperparameters and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ha6oXvmRdDUL"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "CHOSE_LR = 1e-4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=CHOSE_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCPodX8Md-58"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bpvs2GyNmtT4"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n",
    "\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-PrMgRvekuh"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c61ba849c7ec4264827aa44434852f73",
      "cda2970325384486859ecac0df616ce4",
      "b8ea98a75cfe42eb8a2a51a6411f79b9",
      "e0622b90936c48648d26006bd7420f23",
      "75a732e4f22d4f0ab663a7e41310f51f",
      "640c2bb34caa45cbbdbba6bb243fd744",
      "8f13b20c838e41bf88530c0e878e6483",
      "e37e6f4087744687a45c19d5a290c3d0",
      "31b619d6799f4cd49e1b78d05d39d8ca",
      "4e947c0709aa4d898a1bcefa171474e4",
      "6f56bd47eedb4b4b805884a44c243a6d",
      "6ed1a33fa44d4bb08d5080fed8d4f1b2",
      "1229edfaeaff4524a8a2907e885cce92",
      "1414d73e194f43d9b4414f6db9c3db52",
      "a45f76e73be3490e9d5b146416936262",
      "43b4fea5fc6e465fb4d54dbbd4363ee7",
      "da2160d10dc84241a8f919db62c54fad",
      "0d207133018c4864ad3e9308b55d4951",
      "eb5b12b6795744fc95c8ab42136897bc",
      "15ba388344954135ad0edea83b2bcb53",
      "a333d47e8ca343a5a7bf76396fbf98dc",
      "d05daff5a7b74948bb6ddd125bbfd33a",
      "ad6abe81260a4263b9d4d26899dc8b76",
      "3b639384a2f148ada57eec50fd3f4645",
      "f91b8961f9aa448487a819ba4ed18104",
      "53497639cb8c44268422df3b7b4b229a",
      "7c6a7becfb6249f18df26137f89dceb3",
      "76eba4a514d2474b8e73f72afae55d80",
      "945bb0fec33448f19628093d6f84aa65",
      "45d0edee975841098ef94170e26c4c52",
      "96d5fcf0f6844e6bbb42036b974b581d",
      "4c26c33f41774897bbd86aa400fb2ca0",
      "a92dbb9d496d4b99960398c460239ddb",
      "1c2ed27e40da4575a3bfcc8ff9aff498",
      "00ab14fa76194cd2af4aec5922673d8a",
      "2a89b23fb91a439dbb8bb3efdc6d766b",
      "ef7ee5dea37e43eca2e5f29c076310f6",
      "c845d0e1ed554d93af9db70b4e63e8e9",
      "3c3fd209bfea40c59cbfd344b1ddfa26",
      "b83dfd92f3d642e3ba110461be4afa13",
      "9f520afa3ef64011a25b32e526e8651b",
      "a604610183014fadb8fd63feaf05679b",
      "86731033e03747239dc35ed219f95f50",
      "2d4b26fb2fc94d468109c55322748436",
      "7fa884f52dd94b0badc7d1af4bd2c58d",
      "1e251bf067724f30ae404dfc7dc299fc",
      "dbe2b8666ba14f4a96b9c992b2eea4c6",
      "481dc02a58894865956b1aaeac4d52c3",
      "f20d4526e71c4c84ba182ae31e05714c",
      "c10c964ea9c84a7e82a04f0b7992a613",
      "3e14839e05e7497db3adea4090c01c72",
      "7ab2f62d3777424e94b0a1bb2a553e95",
      "095c5d8206dc4d7595a6eb461b3608f8",
      "b10e32b778a146d7a9504242720e6134",
      "e2d64e75f9554c0d8377d6b5aec23994",
      "4328c1f3183e4e78aeb733e97b700795",
      "6aa54b971924428baece45238d65e925",
      "4b654831e8024396bf9bbb1a5c341df5",
      "03e1b654e9dc4bb98ac0997f0ea99910",
      "3f92154f1eee4fef9d65686d7fb700ac",
      "0e3c5fe0b53140408ecd2f45f53aef26",
      "6c79ef5204a04f628e24c7c394a03496",
      "aa2d37a51ace4ae7899c7a74c662174a",
      "f10593bb32bb41959fefb2ec7a851aa6",
      "857dd66f2c5749b8a456360f42e98b21",
      "acea80a49f8043bab9336b309776392d",
      "2c0917b0e71a4245acae15324459ff0d",
      "33302bf5095b4ce2b68924414da935ba",
      "260ecc312cd84798b517c3719c720168",
      "07de302c3b67466183a2c7c843a58877",
      "25c10e5d1e1f48b0a6184357f4663140",
      "ba8e99dd0e5343cfa96da8c4423430ba",
      "87aa3e4e41954e63aa0efd1f2cc9dd5e",
      "c3549b93e91a4795bd69b9567a1d899d",
      "cb29b363cf8943628406fb5c0f7ad2e7",
      "ad2e993b201c4930aebc7b6a99470243",
      "7ff47d1a571040848f7663beeaa3fbb5",
      "c1f2dac20d1b480ea984cc7e74f85b75",
      "27a036afd6c84e8fa3d8aea149ded0f0",
      "a41413ac30714a9b9e2975a18c1c301d",
      "0ee07a59ad444a3991554e75a2054c27",
      "b76743124840488abd6166a294985761",
      "d9507eca956b47c1b09ea43b4767618e",
      "3f90c6b8160a4d8d8ce06841970dc186",
      "9f8b2355b0fd466fa8e9dac3465a0e50",
      "9a8a32ac298440ffa23a2023fa67c9d0",
      "9ae4551059fd46269ed2021e3f21e85f",
      "f5d620722da642a5baa64bf4c7adb51e",
      "2e4443e6e1ed4cf5887bec626239dfeb",
      "da69cdbdc5a440df92f084a0c720e4f5",
      "91f8de1d1b05483db930c044d4e8d528",
      "5fa398ed0be7472388e41168ec555fe2",
      "bac08eed82b74268a25b7e9e9eb17846",
      "73fffc56ecf94e8591b09902f07b71a1",
      "025b924f1c0e46b285331ffc7150e9e3",
      "d5dfaaa32557428ba7965e88d1ff27b6",
      "e820ce71738d45ae917dbd5382154dc1",
      "9b8132ecab7e4af9a6624702fa216154",
      "ee959c4f888b431d817bab8535135358",
      "92eb0f1008df41b9bf4a5985dd10d03e",
      "5e9d0d6702c145ad8721f106ee66c84c",
      "75738ba3074c4262bfb45f30f2ea57bf",
      "e64834ff33514cc7ac939e29bd395e78",
      "498bbb948a7746018993df911985ce89",
      "bb8005b10ece42d9a3b8e517d96f4770",
      "966d3f75ea5744bda249db1077f11a24",
      "fa1f1a96c6574cd38f1eb8b92714160f",
      "ffb5f7d3f2e442fab359063b60c3fed8",
      "47b473a379a546f49b70e7a36809bd9a",
      "848db98aba224cf49ad0f520db778660",
      "84a8dc4380374e1287a9c463a1d2eb17",
      "c42f99bf8916481e8b73c75bf2080872",
      "945d0a5693164cefa9f1ae8c6c2f5e06",
      "94fc226b3c414aeb905798c426f8f93a",
      "8c2c1b88c60f40b780cc9463763fb082",
      "3be53e78e37648c98a5ae8a64f74174f",
      "67ffd1d38d114cb59c9b3711b704cf0f",
      "5059a7bea0254180beed14a279865add",
      "a2d23df9437546f1b37a0fd185f3ac48",
      "bbb64d5186084381952fa091343ef227",
      "b2d24c3516c14737b0b0c2c860adff7c",
      "6e4b43091aa94ba8a181bdf13f971256",
      "847dc7da038540fea1a65ef91f652428",
      "d0a120cecfca43109721ccada3d7aa2a",
      "96ebcf42637f4812b7d13bf92a6d8c34",
      "b0ea46b5472f4cbfb57727b3c34c29bb",
      "526809287dfe4ebab59f6d8248693597",
      "86334910a03a45aba1f4bcd78b80f9eb",
      "9db54b8414af47e1b5ae31c7c7e68ef2",
      "8063695a49ab44feafad33163b9379dc",
      "6e8b68ae06e445bca607c0e378ea30a0",
      "49d9696ac09e495383885cb31f9ef613",
      "09e8a88d6e654fa689477c4e91880ab3",
      "352aa75f6317451f8cc2e6260a24abb1",
      "272752fafd5d4d998b39adb40eee62d8",
      "52eee4866c434f44b17ba8f3a8bd8015",
      "131569b73a8c458786159b3d0fe8b9c3",
      "db0192d509104c64aec58676fce4ee5c",
      "b5d34317a3f74b6fb90ddd0254ae4545",
      "9c5be74307644acdbac63f2825aee620",
      "ddba04d26f7548919854f8b4fba6cf4f",
      "b4b67f7f3a644f71afc18dfab42ccb37",
      "010cc049a0a24028b0aa1fdefc4a1bd7",
      "9ffadc7da2644d348c1b4fc7bda1b512",
      "5df7e0e51b354716840eb58d5e8ff635",
      "0f4f271ec30241298758635145de1342",
      "f442eb43c2784ff3b2b79f2b2ed52a45",
      "4bdbf695c64840a6a975d46160f8fbc6",
      "38bd87420b8449be9e4ac412fddd63c2",
      "b89a266a256f48daa0528c6f76c1d909",
      "1710c72ec40847e2af323199f04149f7",
      "7aa024d9675f4711acb5bbf78b420457",
      "a5df6006f5724d9c813f37f0b19d8de2",
      "6e5ac7a23f4a4aa1bc59086af7df61fd",
      "d94ddbd868444bfcb1ef4517f43f5535",
      "dfc66abe3ba84412a58837cea046155e",
      "d3b29c06d0bd4790a36d3a7de6da391a",
      "9ea04e512c6b40a492359864d0c474ea",
      "c5a75f86a8b848aa90377faf32efeed4",
      "5d12729a354d4d7dbdf32d28bed60a57",
      "536684b5ba354eb6a5adc809630eb934",
      "a01245f94d624e01bca07ff2777c8128",
      "7ba87a8745ae44c9a2ecf6de05969748",
      "3158d53a590545f29187d12df0ebdf6b",
      "25a74c0f2f2f4d74b2df6b72f5aad07f",
      "b01db49661fb401eae2409884749bf42",
      "2fae1f21a53248d8aedbd4f8ec9d5043",
      "706f02cfab9b4ba0aa28089d409dd47c",
      "c5d7d37f1888404f989ff1b8e04bbe49",
      "16ad22a8bd234c7684b295f9476f96ac",
      "30347e06d5b2415f8b0786bdaf4d96d4",
      "87da29dda5d94911bc3c2c904471ef75",
      "af665c0f578747f3aa7035db00efdcbe",
      "2227fd294edd4b57bb1c2401fee7086f",
      "ae6f81f010154e9e8bbd92120c8a8573",
      "552601510c194102a6e114b0500b3a39",
      "3297793d60134c7192ab9cecec738284",
      "9f7e7c5ba9a94b548b490178841c7c85",
      "0646881ec6fd49b08b0e6269775dd178",
      "70c974a70bda419a9262bc4820ae7c3e",
      "d6499166f89e4d31882a5fe9e2a47627",
      "c70f4818f8b64c53bff102bd3e8d6240",
      "1467f206392547988f038b317f80019f",
      "ba9723777d724c30a361ce22463dc9f5",
      "534154293de64ee0b6634eaee25e0bc1",
      "658ae920ee73438580be8da9eb5c18ba",
      "127b3a3a2a9042a4a5dbf6a1a917b838",
      "f9b0d86c28e5461fa6cf975929689726",
      "b85a60ebd3924689b96afd2954aee63c",
      "d0874079656b467d892a0a7f3468f251",
      "4a4150e5f43d4dacb12d2f7effba536d",
      "94e3adcadb514add8432f5d8c0ce659e",
      "e86b0594c9bb420b826bad6722f64f7c",
      "b5e8126fc20045d78c35dcf7e436c463",
      "ed47c5fac432496ea6b3992a74133a45",
      "a1673f4dd961427c97fbcd85dd219e99",
      "863b9cad01ad47a09b2a396c4467bfb3",
      "2ea283eea1ca4c7faec111bdb32bc4dd",
      "373d2cb13a7144b8a61ff428b808a141",
      "1f3ccf9dd88e4149bb39a1d287a69f3d",
      "c38ca5194d8c45618a3fd0fb83d911b6",
      "b3378d96ffc54b53a4fafed5349238b3",
      "fc6f7e0df5b2406992254bb2ccfaa7cd",
      "f4a84d49a0bc4fe6a35696a5110d060b",
      "7c1cffb35d014dbab568f6865e56eeb1",
      "74c3129b1e6d4c00b01276a3257ce65d",
      "64e5f78029224b4ca5c419c6d081e311",
      "f0d30d03a6b04bea90fa0b5e4605e96e",
      "63a9fa60e5d14ec88d5de3b3694c3c19",
      "baa1afa8b56443dbbb6f77ee5d117dce",
      "42eb516c01b14653a0855c25f33a4b40",
      "fd82de73de294a979d140ee1a0591854",
      "16db1eda9b014bff9c9d7f28323cf313",
      "c3d866957c6d41f2a43098b3e1e761ab",
      "3c6c0e50d1314421b5c3ffeb93c7f1e2",
      "0cea3653a2b449fb84890520c700f5bf",
      "9070d6d1d49d4d85bf8386454fb4b9e6",
      "c14ee92d5e0143c89045d03dbc4a0847",
      "d9504d6c3b3943a88fca4513bd8238a4",
      "d1de02ad3d0d4ea69cc45a5ea36577a4",
      "58aab067abaa46b8bac219830182de91",
      "70adeb86519e41df8206ef0b2d12f6a7",
      "22d322d342744651ab1a83afbf4ae0ac",
      "25cf913aba3042bcb028f02890db255e",
      "1a13b0f62ccf4a6298731f522d21a9de",
      "8965ca8cd62f40e6a22ec1ad8327a7f1",
      "b0528b851e474bc69ef742b167e2dc27",
      "cee9966bc0aa4dc2824cf7fdbe35c540",
      "f61eb9187d824e95870bd88675464ae2",
      "19dcb6459ae449598dd8232241fd9f10",
      "1ba94353e6604e2dadd7a004eb1b146f",
      "3904eb22c3f44d6cb924e0849da48c56",
      "483ef6156b564ba1af8c2c9711a7532b",
      "c2a4930c177a4c9f932674d65f6446aa",
      "28eb0772a9c14d74a61ecd1e35406ee4",
      "a300d8058d0c499a9aa9f01b73107342",
      "f3c4bdde72224a7d93746cf21e3724f5",
      "3892383362c049b2bc2602c0e1d147c7",
      "0e95c480f00b4feea0efaf61a7f79bc7",
      "c71ad9c63a504747b3b48097a6529bd7",
      "986b51163e73469da8d426b945a1830a",
      "243385eac4764357b930be0d4498e9a6",
      "b1e867d72bd74891a55f7eac76b83b6d",
      "190f6e404bd74a6ebb04b23f527f97cd",
      "774124c7d7264c719b78f6a1b5b93e7d",
      "3ab95e8b209d484183b32a64522e6010",
      "8482a76550aa49e4bb5bb2ea4b720e37",
      "3965db2be42b4fc2aeedbd46ea917648",
      "aa0b82f79bb04fb9bdc5cc3d6cf6c052",
      "ea280ff057b64f259f73432ad1a89673",
      "8ba54c0968204674a4fd1522b27b66e4",
      "a326b0a621424e769eed3e7496de93bb",
      "35854136bb0e4a3fbf8b588aa0d61074",
      "6df10c3c056a41e9a46c057c60605c85",
      "e41b0c0cce3a47f4bdae66d64b825660",
      "851da3d697c349eba6e21c4ff5a853ce",
      "6fad7d26a6d14033ba11ed34d18d0dfc",
      "3b08bc468f1e4d5ab6c3a9553888dc36",
      "f0984f4a6e684b75afa2a4ece2df3a6d",
      "4a019f5aa351412cb133a3d9103e9791",
      "c5bc15272df14a64ac84c36c959bd5c2",
      "669f777d748a45bab6761fd3e6df69ff",
      "3d2bee9f38684012ba313cb32ad73340",
      "f0d7d65f90f54d4eb903fc078ebe9a30",
      "bce86d081bf24cb993a03edd7feabd35",
      "6d8b25e1aeab4e47b15fa6e330d5e724",
      "47734187de7d4e9995b968ef3b2705a4",
      "1b495a3903254c668f4ce300addec262",
      "c542db67a4624fc7a67d32d3c10b092e",
      "95e9977de0004694affe786d12770a51",
      "0319bef0bb3544ba9cae104441036280",
      "43748311b8e84a61bb7dc30828af1a27",
      "33ede512f4944c7d946d0886b7655765",
      "9fe900a6341c4b7d82228c01e843f894",
      "2683ac9a5fec472e80353ba7a91e2e36"
     ]
    },
    "id": "jPFaZa0Am-aN",
    "outputId": "a48c1d4a-7dfb-441d-fdfa-ac39e28d74b0"
   },
   "outputs": [],
   "source": [
    "!pip install early-stopping-pytorch\n",
    "from early_stopping_pytorch import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=7, verbose=True)\n",
    "\n",
    "EPOCHS = 20 # You may need to run longer to get better results. Lab example is for demo only\n",
    "\n",
    "# keeping-track-of-losses\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in trange(EPOCHS, desc=\"Epochs\"):\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iter, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iter, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "\n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSaOuT-gv0wc"
   },
   "source": [
    "Example code to plot losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "7su0E10QsoLG",
    "outputId": "16c0f4b3-a8a2-4840-87de-06e65a9cf7df"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses, label='Validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93LCuMfKwOXX"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, iterator, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            y_prob = F.softmax(y_pred, dim=-1)\n",
    "\n",
    "            images.append(x.cpu())\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim=0)\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    probs = torch.cat(probs, dim=0)\n",
    "\n",
    "    return images, labels, probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLg-UGiOztuD"
   },
   "source": [
    "Loading the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rm0fXD2ltfoH"
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('best-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq_2cz0xgwtm"
   },
   "outputs": [],
   "source": [
    "images, labels, probs = get_predictions(model, test_iter, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1S4ee_3g_Oy"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/content/drive/My Drive/Deep Learning/ISIC_balanced_split/ISIC2017_Test_GroundTruth.csv')\n",
    "y_test = np.array(test.drop(['id'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 925
    },
    "id": "H6utF06dhZ6o",
    "outputId": "b82a28e1-3813-45bd-d283-3eb4e3436d5a"
   },
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "#change to OneVsOneClassifier for generalized AUC\n",
    "#from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "#from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "n_classes=2\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), probs.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[0], tpr[0], color='green', lw=lw,\n",
    "         label='ROC curve of Melanoma (area = {1:0.2f})'\n",
    "         ''.format(0, roc_auc[0]))\n",
    "\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange', lw=lw,\n",
    "         label='ROC curve of Other (area = {1:0.2f})'\n",
    "         ''.format(1, roc_auc[1]))\n",
    "\n",
    "# if you need more classes\n",
    "# plt.plot(fpr[2], tpr[2], color='red', lw=lw,\n",
    "#          label='ROC curve of third class (area = {1:0.2f})'\n",
    "#          ''.format(1, roc_auc[2]))\n",
    "\n",
    "# plt.plot(fpr[3], tpr[3], color='cornflowerblue', lw=lw,\n",
    "#          label='ROC curve of fourth class (area = {1:0.2f})'\n",
    "#          ''.format(1, roc_auc[3]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curves of Proposed Method')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ly7GDpRritFY",
    "outputId": "12dd9c66-be03-43b2-bda8-2dfa45a2e45d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from matplotlib.colors import Normalize\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "pred_class = np.argmax(probs,axis=1)\n",
    "true_class = np.argmax(y_test, axis=1)\n",
    "\n",
    "cm=confusion_matrix(true_class, pred_class)\n",
    "class_names = unique_labels(true_class, pred_class)\n",
    "print(cm)\n",
    "print(class_names)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "C = cm\n",
    "divisor = np.zeros((2,2))\n",
    "divisor[0][:] = 117\n",
    "divisor[1][:] = 483\n",
    "cm_normalised=np.divide(cm, divisor)\n",
    "print(np.transpose(C.sum(axis=1)))\n",
    "print(divisor)\n",
    "cm_normalised = np.round(cm_normalised, 2)\n",
    "disp = ConfusionMatrixDisplay(cm_normalised, display_labels=class_names)\n",
    "disp.plot(cmap=plt.cm.Blues, values_format='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FMdlbRvPNR9Z",
    "outputId": "d892e2d6-6feb-4abd-b7ec-bbad6f5b5eb7"
   },
   "outputs": [],
   "source": [
    "TP = np.diag(cm)\n",
    "FP = cm.sum(axis=0) - TP\n",
    "FN = cm.sum(axis=1) - TP\n",
    "\n",
    "precision_per_class = TP / (TP + FP)\n",
    "recall_per_class = TP / (TP + FN)\n",
    "f1_per_class = 2 * (precision_per_class * recall_per_class) / (precision_per_class + recall_per_class)\n",
    "\n",
    "weights = cm.sum(axis=1) / cm.sum()\n",
    "precision_weighted = np.sum(precision_per_class * weights)\n",
    "recall_weighted = np.sum(recall_per_class * weights)\n",
    "f1_weighted = np.sum(f1_per_class * weights)\n",
    "\n",
    "\n",
    "print(f\"Precision (Weighted): {precision_weighted:.2f}\")\n",
    "print(f\"Recall (Weighted): {recall_weighted:.2f}\")\n",
    "print(f\"F1-score (Weighted): {f1_weighted:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
